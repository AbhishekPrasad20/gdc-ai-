{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e496977d",
   "metadata": {},
   "source": [
    "# Developer Role Classification: Data Preprocessing\n",
    "\n",
    "This notebook documents the complete preprocessing pipeline for the Developer Role Classification project. We'll transform raw developer commit data into model-ready examples through a series of well-documented steps.\n",
    "\n",
    "The preprocessing pipeline includes:\n",
    "1. Data loading and inspection\n",
    "2. Data cleaning\n",
    "3. Feature engineering\n",
    "4. Data normalization/scaling\n",
    "5. Train-test split\n",
    "6. Data augmentation (if needed)\n",
    "7. Saving the processed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8bdc2c",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Inspection\n",
    "\n",
    "First, let's import the necessary libraries and load our raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab13ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "# Print environment information for reproducibility\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NLTK version: {nltk.__version__}\")\n",
    "\n",
    "# For reproducibility - Set fixed seeds everywhere\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Try to set seeds for other libraries if they're installed\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not installed\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed\")\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the raw dataset\n",
    "# Assuming the raw data is stored in a CSV file with commit messages and metadata\n",
    "try:\n",
    "    raw_data = pd.read_csv('raw_developer_commits.csv')\n",
    "    print(f\"Dataset loaded successfully with {raw_data.shape[0]} rows and {raw_data.shape[1]} columns.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, we'll create a sample dataset for demonstration\n",
    "    print(\"Raw data file not found. Creating a sample dataset for demonstration.\")\n",
    "    \n",
    "    # Sample data with commit messages from different developer roles\n",
    "    sample_data = {\n",
    "        'commit_id': [f'commit_{i}' for i in range(1, 101)],\n",
    "        'author': [f'dev_{np.random.randint(1, 20)}' for _ in range(100)],\n",
    "        'timestamp': pd.date_range(start='2020-01-01', periods=100, freq='D'),\n",
    "        'commit_message': [\n",
    "            \"Fixed bug in authentication module\",\n",
    "            \"Added new UI components for dashboard\",\n",
    "            \"Optimized database queries for better performance\",\n",
    "            \"Updated documentation for API endpoints\",\n",
    "            \"Implemented CI/CD pipeline configuration\",\n",
    "            \"Refactored user service for better maintainability\",\n",
    "            \"Created unit tests for payment processing\",\n",
    "            \"Fixed security vulnerability in login form\",\n",
    "            \"Integrated third-party analytics service\",\n",
    "            \"Deployed new version to production server\"\n",
    "        ] * 10,\n",
    "        'files_changed': [np.random.randint(1, 10) for _ in range(100)],\n",
    "        'insertions': [np.random.randint(10, 200) for _ in range(100)],\n",
    "        'deletions': [np.random.randint(5, 100) for _ in range(100)],\n",
    "        'role': np.random.choice(['Frontend', 'Backend', 'DevOps', 'Full Stack', 'QA'], 100)\n",
    "    }\n",
    "    \n",
    "    raw_data = pd.DataFrame(sample_data)\n",
    "    print(f\"Sample dataset created with {raw_data.shape[0]} rows and {raw_data.shape[1]} columns.\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "display(raw_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f949536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic information about the dataset\n",
    "print(\"Dataset info:\")\n",
    "raw_data.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(raw_data.isnull().sum())\n",
    "\n",
    "# Basic statistics for numerical columns\n",
    "print(\"\\nBasic statistics for numerical columns:\")\n",
    "display(raw_data.describe())\n",
    "\n",
    "# Distribution of developer roles (target variable)\n",
    "print(\"\\nDistribution of developer roles:\")\n",
    "role_counts = raw_data['role'].value_counts()\n",
    "display(role_counts)\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y=raw_data['role'])\n",
    "plt.title('Distribution of Developer Roles')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Role')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b8965",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "Now let's clean our dataset by:\n",
    "1. Handling missing values\n",
    "2. Removing duplicates\n",
    "3. Standardizing text data\n",
    "4. Removing outliers if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numeric columns, fill with median\n",
    "numeric_cols = raw_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numeric_cols:\n",
    "    if raw_data[col].isnull().sum() > 0:\n",
    "        raw_data[col] = raw_data[col].fillna(raw_data[col].median())\n",
    "\n",
    "# For text columns, fill with empty string\n",
    "text_cols = raw_data.select_dtypes(include=['object']).columns\n",
    "for col in text_cols:\n",
    "    if col != 'role':  # Don't fill missing values in the target column\n",
    "        if raw_data[col].isnull().sum() > 0:\n",
    "            raw_data[col] = raw_data[col].fillna('')\n",
    "\n",
    "# Check if we have any missing values left\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(raw_data.isnull().sum())\n",
    "\n",
    "# Remove duplicate entries based on commit_id\n",
    "duplicates = raw_data.duplicated(subset=['commit_id'])\n",
    "print(f\"\\nNumber of duplicate entries: {duplicates.sum()}\")\n",
    "raw_data = raw_data.drop_duplicates(subset=['commit_id'])\n",
    "print(f\"Dataset shape after removing duplicates: {raw_data.shape}\")\n",
    "\n",
    "# Standardize text in commit messages\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to commit messages\n",
    "raw_data['clean_commit_message'] = raw_data['commit_message'].apply(clean_text)\n",
    "\n",
    "# Display sample of cleaned messages\n",
    "print(\"\\nSample of original vs. cleaned commit messages:\")\n",
    "sample_comparison = pd.DataFrame({\n",
    "    'Original': raw_data['commit_message'].head(),\n",
    "    'Cleaned': raw_data['clean_commit_message'].head()\n",
    "})\n",
    "display(sample_comparison)\n",
    "\n",
    "# Check for outliers in numerical columns\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    if col != 'commit_id':  # Skip non-meaningful numeric IDs\n",
    "        plt.subplot(1, len(numeric_cols)-1, i+1)\n",
    "        sns.boxplot(y=raw_data[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Remove extreme outliers (optional)\n",
    "# Here we're using IQR method to identify outliers\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Apply outlier removal to numerical columns if needed\n",
    "# This is commented out by default as it might remove important data points\n",
    "# for col in ['files_changed', 'insertions', 'deletions']:\n",
    "#     raw_data = remove_outliers(raw_data, col)\n",
    "# print(f\"Dataset shape after outlier removal: {raw_data.shape}\")\n",
    "\n",
    "# Clean data after preprocessing\n",
    "clean_data = raw_data.copy()\n",
    "print(f\"\\nClean dataset shape: {clean_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150d191",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "In this section, we'll:\n",
    "1. Extract features from commit messages\n",
    "2. Create new features from existing metadata\n",
    "3. Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82688493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up text processing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to process text with stopword removal and lemmatization\n",
    "def process_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    processed = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(processed)\n",
    "\n",
    "# Apply text processing\n",
    "clean_data['processed_message'] = clean_data['clean_commit_message'].apply(process_text)\n",
    "\n",
    "# Extract features from commit messages\n",
    "# 1. Message length\n",
    "clean_data['message_length'] = clean_data['commit_message'].apply(lambda x: len(str(x)))\n",
    "clean_data['word_count'] = clean_data['processed_message'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# 2. Check for keywords related to different roles\n",
    "frontend_keywords = ['ui', 'css', 'html', 'react', 'angular', 'vue', 'component', 'style', 'interface']\n",
    "backend_keywords = ['api', 'database', 'query', 'endpoint', 'server', 'service', 'model', 'controller']\n",
    "devops_keywords = ['deploy', 'pipeline', 'docker', 'kubernetes', 'ci', 'cd', 'configuration', 'infrastructure']\n",
    "qa_keywords = ['test', 'bug', 'fix', 'issue', 'verify', 'validation', 'coverage', 'quality']\n",
    "\n",
    "def count_keywords(text, keyword_list):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    text = text.lower()\n",
    "    return sum(1 for keyword in keyword_list if keyword in text)\n",
    "\n",
    "clean_data['frontend_keywords'] = clean_data['processed_message'].apply(lambda x: count_keywords(x, frontend_keywords))\n",
    "clean_data['backend_keywords'] = clean_data['processed_message'].apply(lambda x: count_keywords(x, backend_keywords))\n",
    "clean_data['devops_keywords'] = clean_data['processed_message'].apply(lambda x: count_keywords(x, devops_keywords))\n",
    "clean_data['qa_keywords'] = clean_data['processed_message'].apply(lambda x: count_keywords(x, qa_keywords))\n",
    "\n",
    "# 3. Create day of week and hour features from timestamp\n",
    "if pd.api.types.is_datetime64_any_dtype(clean_data['timestamp']):\n",
    "    clean_data['day_of_week'] = clean_data['timestamp'].dt.dayofweek\n",
    "    clean_data['hour_of_day'] = clean_data['timestamp'].dt.hour\n",
    "elif isinstance(clean_data['timestamp'][0], str):\n",
    "    clean_data['timestamp'] = pd.to_datetime(clean_data['timestamp'])\n",
    "    clean_data['day_of_week'] = clean_data['timestamp'].dt.dayofweek\n",
    "    clean_data['hour_of_day'] = clean_data['timestamp'].dt.hour\n",
    "\n",
    "# 4. Calculate ratio features\n",
    "clean_data['insertion_deletion_ratio'] = clean_data['insertions'] / (clean_data['deletions'] + 1)  # +1 to avoid division by zero\n",
    "\n",
    "# 5. One-hot encode the role (target variable)\n",
    "# We'll use pandas get_dummies for simplicity\n",
    "role_encoded = pd.get_dummies(clean_data['role'], prefix='role')\n",
    "clean_data = pd.concat([clean_data, role_encoded], axis=1)\n",
    "\n",
    "# Display the new features\n",
    "print(\"Dataset with engineered features:\")\n",
    "display(clean_data.head())\n",
    "\n",
    "# Check correlation between engineered features and roles\n",
    "# Create a correlation matrix for the new features and encoded roles\n",
    "feature_cols = ['message_length', 'word_count', 'frontend_keywords', 'backend_keywords', \n",
    "                'devops_keywords', 'qa_keywords', 'files_changed', 'insertions', 'deletions',\n",
    "                'insertion_deletion_ratio']\n",
    "\n",
    "role_cols = [col for col in clean_data.columns if col.startswith('role_')]\n",
    "corr_cols = feature_cols + role_cols\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = clean_data[corr_cols].corr()\n",
    "\n",
    "# Plot heatmap of correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f85c9",
   "metadata": {},
   "source": [
    "## 4. Data Normalization/Scaling\n",
    "\n",
    "Now we'll normalize or standardize our numerical features to ensure they're on comparable scales for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46269164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Select numerical features for scaling\n",
    "numerical_features = ['message_length', 'word_count', 'frontend_keywords', 'backend_keywords', \n",
    "                      'devops_keywords', 'qa_keywords', 'files_changed', 'insertions', 'deletions',\n",
    "                      'insertion_deletion_ratio', 'day_of_week', 'hour_of_day']\n",
    "\n",
    "# Create a copy of the data for scaling\n",
    "scaled_data = clean_data.copy()\n",
    "\n",
    "# Apply StandardScaler (z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "scaled_data[numerical_features] = scaler.fit_transform(scaled_data[numerical_features])\n",
    "\n",
    "# Display scaled data\n",
    "print(\"Scaled numerical features (first 5 rows):\")\n",
    "display(scaled_data[numerical_features].head())\n",
    "\n",
    "# Visualize distribution of scaled features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    sns.histplot(scaled_data[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the scaler for later use\n",
    "import joblib\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "print(\"Scaler saved to 'feature_scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b1343",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split\n",
    "\n",
    "Split the processed data into training and testing sets, ensuring balanced class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target\n",
    "X = scaled_data[numerical_features]\n",
    "y = clean_data['role']  # Original categorical target\n",
    "\n",
    "# Split into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Verify the split\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution in train and test sets\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "display(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in testing set:\")\n",
    "display(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Create a DataFrame with the original text and the features for both train and test sets\n",
    "train_indices = y_train.index\n",
    "test_indices = y_test.index\n",
    "\n",
    "train_data = scaled_data.loc[train_indices].copy()\n",
    "test_data = scaled_data.loc[test_indices].copy()\n",
    "\n",
    "print(f\"\\nFinal training data shape: {train_data.shape}\")\n",
    "print(f\"Final testing data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c979a3",
   "metadata": {},
   "source": [
    "## 6. Data Augmentation (if needed)\n",
    "\n",
    "Check if we need data augmentation to address class imbalance. If so, we'll use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic examples for minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class imbalance\n",
    "class_counts = y_train.value_counts()\n",
    "print(\"Class distribution before augmentation:\")\n",
    "display(class_counts)\n",
    "\n",
    "# Calculate imbalance ratio (majority class / minority class)\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "# If the imbalance ratio is high (e.g., > 1.5), apply SMOTE\n",
    "if imbalance_ratio > 1.5:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "    print(\"\\nApplying SMOTE to balance the classes...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"Original training set shape: {X_train.shape}\")\n",
    "    print(f\"Resampled training set shape: {X_train_resampled.shape}\")\n",
    "    \n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    display(pd.Series(y_train_resampled).value_counts())\n",
    "    \n",
    "    # Update the training data\n",
    "    X_train = X_train_resampled\n",
    "    y_train = y_train_resampled\n",
    "else:\n",
    "    print(\"\\nClass distribution is relatively balanced. No augmentation needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b1059",
   "metadata": {},
   "source": [
    "## 7. Save Processed Dataset\n",
    "\n",
    "Finally, let's save our processed dataset for use in the modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fca421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and target into final datasets\n",
    "final_train = pd.DataFrame(X_train, index=X_train.index)\n",
    "final_train['role'] = y_train\n",
    "\n",
    "final_test = pd.DataFrame(X_test, index=X_test.index)\n",
    "final_test['role'] = y_test\n",
    "\n",
    "# Add back the original text for reference\n",
    "final_train['commit_message'] = clean_data.loc[final_train.index, 'commit_message']\n",
    "final_test['commit_message'] = clean_data.loc[final_test.index, 'commit_message']\n",
    "\n",
    "# Save the processed datasets\n",
    "final_dataset = pd.concat([final_train, final_test])\n",
    "final_dataset.to_csv('final_dataset.csv', index=False)\n",
    "print(\"Final processed dataset saved to 'final_dataset.csv'\")\n",
    "\n",
    "# Save train and test sets separately\n",
    "final_train.to_csv('train_dataset.csv', index=False)\n",
    "final_test.to_csv('test_dataset.csv', index=False)\n",
    "print(\"Training dataset saved to 'train_dataset.csv'\")\n",
    "print(\"Testing dataset saved to 'test_dataset.csv'\")\n",
    "\n",
    "# Save the processed dataset in pickle format for preserving data types\n",
    "import pickle\n",
    "\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'feature_names': numerical_features,\n",
    "        'scaler': scaler\n",
    "    }, f)\n",
    "\n",
    "print(\"Processed data saved in pickle format to 'processed_data.pkl'\")\n",
    "\n",
    "# Create a preprocessing script that can be used for new data\n",
    "with open('preprocess.py', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "def preprocess_commit_data(data):\n",
    "    # Download NLTK resources if needed\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "    try:\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "    except LookupError:\n",
    "        nltk.download('wordnet')\n",
    "    \n",
    "    # Clean text\n",
    "    data['clean_commit_message'] = data['commit_message'].apply(clean_text)\n",
    "    \n",
    "    # Process text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data['processed_message'] = data['clean_commit_message'].apply(\n",
    "        lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x) \n",
    "                           if word not in stop_words])\n",
    "    )\n",
    "    \n",
    "    # Extract features\n",
    "    data['message_length'] = data['commit_message'].apply(lambda x: len(str(x)))\n",
    "    data['word_count'] = data['processed_message'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Keyword counts\n",
    "    frontend_keywords = ['ui', 'css', 'html', 'react', 'angular', 'vue', 'component', 'style', 'interface']\n",
    "    backend_keywords = ['api', 'database', 'query', 'endpoint', 'server', 'service', 'model', 'controller']\n",
    "    devops_keywords = ['deploy', 'pipeline', 'docker', 'kubernetes', 'ci', 'cd', 'configuration', 'infrastructure']\n",
    "    qa_keywords = ['test', 'bug', 'fix', 'issue', 'verify', 'validation', 'coverage', 'quality']\n",
    "    \n",
    "    data['frontend_keywords'] = data['processed_message'].apply(lambda x: count_keywords(x, frontend_keywords))\n",
    "    data['backend_keywords'] = data['processed_message'].apply(lambda x: count_keywords(x, backend_keywords))\n",
    "    data['devops_keywords'] = data['processed_message'].apply(lambda x: count_keywords(x, devops_keywords))\n",
    "    data['qa_keywords'] = data['processed_message'].apply(lambda x: count_keywords(x, qa_keywords))\n",
    "    \n",
    "    # Time features\n",
    "    if pd.api.types.is_datetime64_any_dtype(data['timestamp']):\n",
    "        data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "        data['hour_of_day'] = data['timestamp'].dt.hour\n",
    "    elif 'timestamp' in data.columns:\n",
    "        try:\n",
    "            data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "            data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "            data['hour_of_day'] = data['timestamp'].dt.hour\n",
    "        except:\n",
    "            data['day_of_week'] = 0\n",
    "            data['hour_of_day'] = 0\n",
    "    else:\n",
    "        data['day_of_week'] = 0\n",
    "        data['hour_of_day'] = 0\n",
    "    \n",
    "    # Ratio features\n",
    "    if 'insertions' in data.columns and 'deletions' in data.columns:\n",
    "        data['insertion_deletion_ratio'] = data['insertions'] / (data['deletions'] + 1)\n",
    "    else:\n",
    "        data['insertion_deletion_ratio'] = 0\n",
    "    \n",
    "    # Select features\n",
    "    features = ['message_length', 'word_count', 'frontend_keywords', 'backend_keywords', \n",
    "                'devops_keywords', 'qa_keywords', 'files_changed', 'insertions', 'deletions',\n",
    "                'insertion_deletion_ratio', 'day_of_week', 'hour_of_day']\n",
    "    \n",
    "    # Make sure all required columns exist\n",
    "    for feature in features:\n",
    "        if feature not in data.columns:\n",
    "            data[feature] = 0\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = joblib.load('feature_scaler.pkl')\n",
    "    data[features] = scaler.transform(data[features])\n",
    "    \n",
    "    return data, features\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\\\S+|www\\\\S+|https\\\\S+', '', text)\n",
    "    text = re.sub(r'\\\\W', ' ', text)\n",
    "    text = re.sub(r'\\\\d', ' ', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def count_keywords(text, keyword_list):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    text = text.lower()\n",
    "    return sum(1 for keyword in keyword_list if keyword in text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    data = pd.read_csv('raw_developer_commits.csv')\n",
    "    processed_data, features = preprocess_commit_data(data)\n",
    "    processed_data.to_csv('processed_dataset.csv', index=False)\n",
    "    print(f\"Processed {len(processed_data)} commits and saved to 'processed_dataset.csv'\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"Preprocessing script saved to 'preprocess.py'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a097836",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've processed raw developer commit data into model-ready examples through:\n",
    "\n",
    "1. **Data Loading and Inspection**: Loaded the raw data, examined its structure, and identified data quality issues.\n",
    "\n",
    "2. **Data Cleaning**: Handled missing values, removed duplicates, standardized text, and addressed outliers.\n",
    "\n",
    "3. **Feature Engineering**: Created meaningful features from commit messages and metadata, including:\n",
    "   - Text-based features: message length, word count, keyword counts\n",
    "   - Time-based features: day of week, hour of day\n",
    "   - Ratio-based features: insertion/deletion ratio\n",
    "\n",
    "4. **Data Normalization**: Standardized numerical features to ensure they're on comparable scales.\n",
    "\n",
    "5. **Train-Test Split**: Split the data into training and testing sets with stratification to maintain class distribution.\n",
    "\n",
    "6. **Data Augmentation**: Applied SMOTE for class balancing when needed.\n",
    "\n",
    "7. **Saved Processed Dataset**: Saved the final processed dataset and created a reusable preprocessing script.\n",
    "\n",
    "The final dataset is now ready for modeling and contains engineered features that capture the essence of developer commit patterns."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
