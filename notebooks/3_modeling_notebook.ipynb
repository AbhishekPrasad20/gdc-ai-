{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f73eee",
   "metadata": {},
   "source": [
    "# Developer Role Classification: Modeling\n",
    "\n",
    "This notebook contains the modeling process for classifying developer roles based on commit data. We'll implement and evaluate multiple models, starting with baselines and progressing to more sophisticated approaches.\n",
    "\n",
    "## Objectives:\n",
    "1. Implement baseline models for developer role classification\n",
    "2. Develop improved models with optimized hyperparameters\n",
    "3. Compare model performance across various metrics\n",
    "4. Select the best model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import joblib\n",
    "\n",
    "# Print environment information for reproducibility\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"XGBoost version: {xgboost.__version__}\")\n",
    "\n",
    "# For reproducibility - Set fixed seeds everywhere\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set seeds for various libraries\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not installed\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed\")\n",
    "\n",
    "# Set Python hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "# Ensure reproducibility in sklearn\n",
    "from sklearn.utils import check_random_state\n",
    "check_random_state(RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684a3a6",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "First, let's load our processed dataset and prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed dataset\n",
    "try:\n",
    "    data = pd.read_csv('final_dataset.csv')\n",
    "    print(f\"Dataset loaded successfully with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Please run the preprocessing notebook first.\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "display(data.head())\n",
    "\n",
    "# Identify features and target\n",
    "# Exclude non-feature columns\n",
    "exclude_cols = ['role', 'commit_message', 'processed_message', 'clean_commit_message']\n",
    "if 'processed_for_cloud' in data.columns:\n",
    "    exclude_cols.append('processed_for_cloud')\n",
    "\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "target_col = 'role'\n",
    "\n",
    "print(f\"\\nFeatures to be used ({len(feature_cols)}):\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Prepare X and y\n",
    "X = data[feature_cols].values\n",
    "y = data[target_col].values\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "print(f\"\\nTarget classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Split into train, validation, and test sets (60%, 20%, 20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y_encoded, test_size=0.4, random_state=42, stratify=y_encoded)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Check class distribution in each set\n",
    "def print_class_distribution(y, set_name):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"\\nClass distribution in {set_name} set:\")\n",
    "    for i, cls in enumerate(label_encoder.classes_):\n",
    "        print(f\"  {cls}: {counts[counts.argsort()[i]]}\")\n",
    "\n",
    "print_class_distribution(y_train, \"training\")\n",
    "print_class_distribution(y_val, \"validation\")\n",
    "print_class_distribution(y_test, \"test\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abc0b6",
   "metadata": {},
   "source": [
    "## 2. Evaluation Functions\n",
    "\n",
    "Define functions to evaluate model performance using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, label_encoder, set_name=\"\"):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    # Calculate class-wise precision, recall, and F1\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y, y_pred, average=None)\n",
    "    \n",
    "    # Calculate macro F1\n",
    "    macro_f1 = f1_score(y, y_pred, average='macro')\n",
    "    \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y, y_pred, target_names=label_encoder.classes_, output_dict=True)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': support,\n",
    "        'class_report': class_report,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_metrics_comparison(metrics_dict, metric_name):\n",
    "    \"\"\"Plot comparison of a specific metric across models.\"\"\"\n",
    "    models = list(metrics_dict.keys())\n",
    "    values = [metrics_dict[model][metric_name] for model in models]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(models, values, color='skyblue')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title(f'Comparison of {metric_name.capitalize()} Across Models')\n",
    "    plt.ylabel(metric_name.capitalize())\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, max(values) + 0.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def log_results(model_name, metrics, classes):\n",
    "    \"\"\"Print model evaluation results.\"\"\"\n",
    "    print(f\"=== {model_name} Performance ===\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class Performance:\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        print(f\"  {cls}:\")\n",
    "        print(f\"    Precision: {metrics['precision'][i]:.4f}\")\n",
    "        print(f\"    Recall: {metrics['recall'][i]:.4f}\")\n",
    "        print(f\"    F1: {metrics['f1'][i]:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    plot_confusion_matrix(metrics['confusion_matrix'], classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fe0d5",
   "metadata": {},
   "source": [
    "## 3. Baseline Models\n",
    "\n",
    "Let's implement and evaluate baseline models to establish a performance benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store model results\n",
    "model_results = {}\n",
    "\n",
    "# Baseline 1: Stratified Random Classifier\n",
    "baseline_stratified = DummyClassifier(strategy='stratified', random_state=42)\n",
    "baseline_stratified.fit(X_train_scaled, y_train)\n",
    "baseline_metrics = evaluate_model(baseline_stratified, X_val_scaled, y_val, label_encoder)\n",
    "log_results(\"Baseline (Stratified Random)\", baseline_metrics, label_encoder.classes_)\n",
    "model_results[\"Baseline (Stratified)\"] = baseline_metrics\n",
    "\n",
    "# Baseline 2: Most Frequent Class Classifier\n",
    "baseline_most_frequent = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "baseline_most_frequent.fit(X_train_scaled, y_train)\n",
    "baseline_metrics = evaluate_model(baseline_most_frequent, X_val_scaled, y_val, label_encoder)\n",
    "log_results(\"Baseline (Most Frequent)\", baseline_metrics, label_encoder.classes_)\n",
    "model_results[\"Baseline (Most Frequent)\"] = baseline_metrics\n",
    "\n",
    "# Baseline 3: Simple Logistic Regression\n",
    "baseline_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_lr.fit(X_train_scaled, y_train)\n",
    "baseline_metrics = evaluate_model(baseline_lr, X_val_scaled, y_val, label_encoder)\n",
    "log_results(\"Baseline (Logistic Regression)\", baseline_metrics, label_encoder.classes_)\n",
    "model_results[\"Baseline (Logistic Regression)\"] = baseline_metrics\n",
    "\n",
    "# Compare baseline models\n",
    "plot_metrics_comparison(model_results, 'macro_f1')\n",
    "plot_metrics_comparison(model_results, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad593c1",
   "metadata": {},
   "source": [
    "## 4. Advanced Models\n",
    "\n",
    "Now let's implement more sophisticated models with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Random Forest\n",
    "print(\"Training Random Forest model...\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    rf, param_grid, cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Print best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best macro F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_metrics = evaluate_model(best_rf, X_val_scaled, y_val, label_encoder)\n",
    "log_results(\"Random Forest (Optimized)\", rf_metrics, label_encoder.classes_)\n",
    "model_results[\"Random Forest\"] = rf_metrics\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': best_rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bf145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Gradient Boosting\n",
    "print(\"Training Gradient Boosting model...\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    gb, param_grid, cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_gb = grid_search.best_estimator_\n",
    "\n",
    "# Print best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best macro F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "gb_metrics = evaluate_model(best_gb, X_val_scaled, y_val, label_encoder)\n",
    "log_results(\"Gradient Boosting (Optimized)\", gb_metrics, label_encoder.classes_)\n",
    "model_results[\"Gradient Boosting\"] = gb_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 XGBoost\n",
    "print(\"Training XGBoost model...\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_child_weight': [1, 3]\n",
    "}\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "grid_search = GridSearchCV(\n",
    "    xgb, param_grid, cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Print best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best macro F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "xgb_metrics = evaluate_model(best_xgb, X_val_scaled, y_val, label_encoder)\n",
    "log_results(\"XGBoost (Optimized)\", xgb_metrics, label_encoder.classes_)\n",
    "model_results[\"XGBoost\"] = xgb_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa10abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Support Vector Machine\n",
    "print(\"Training SVM model...\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "svm = SVC(probability=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    svm, param_grid, cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Print best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best macro F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "svm_metrics = evaluate_model(best_svm, X_val_scaled, y_val, label_encoder)\n",
    "log_results(\"SVM (Optimized)\", svm_metrics, label_encoder.classes_)\n",
    "model_results[\"SVM\"] = svm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c519900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Neural Network (MLP)\n",
    "print(\"Training Neural Network model...\")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    mlp, param_grid, cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_mlp = grid_search.best_estimator_\n",
    "\n",
    "# Print best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best macro F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "mlp_metrics = evaluate_model(best_mlp, X_val_scaled, y_val, label_encoder)\n",
    "log_results(\"Neural Network (Optimized)\", mlp_metrics, label_encoder.classes_)\n",
    "model_results[\"Neural Network\"] = mlp_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd494376",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Selection\n",
    "\n",
    "Compare all models and select the best performing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"Model Comparison (Validation Set):\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(model_results.keys()),\n",
    "    'Accuracy': [model_results[m]['accuracy'] for m in model_results],\n",
    "    'Macro F1': [model_results[m]['macro_f1'] for m in model_results]\n",
    "}).sort_values('Macro F1', ascending=False)\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "# Plot comparison of macro F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='Macro F1', data=results_df)\n",
    "plt.title('Comparison of Macro F1 Scores Across Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot comparison of accuracy scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=results_df)\n",
    "plt.title('Comparison of Accuracy Scores Across Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best model based on macro F1 score\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"\\nBest model based on Macro F1 score: {best_model_name}\")\n",
    "\n",
    "# Get corresponding model object\n",
    "if best_model_name == \"Random Forest\":\n",
    "    best_model = best_rf\n",
    "elif best_model_name == \"Gradient Boosting\":\n",
    "    best_model = best_gb\n",
    "elif best_model_name == \"XGBoost\":\n",
    "    best_model = best_xgb\n",
    "elif best_model_name == \"SVM\":\n",
    "    best_model = best_svm\n",
    "elif best_model_name == \"Neural Network\":\n",
    "    best_model = best_mlp\n",
    "else:\n",
    "    # Default to the best baseline if somehow a baseline is best\n",
    "    best_model = baseline_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14682c6e",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation on Test Set\n",
    "\n",
    "Evaluate the best model on the test set to get the final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce13988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set\n",
    "print(f\"Evaluating {best_model_name} on test set...\")\n",
    "test_metrics = evaluate_model(best_model, X_test_scaled, y_test, label_encoder, \"test\")\n",
    "log_results(f\"{best_model_name} (Test Set)\", test_metrics, label_encoder.classes_)\n",
    "\n",
    "# Compare validation and test performance\n",
    "print(\"\\nValidation vs Test Performance:\")\n",
    "val_performance = model_results[best_model_name]\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Macro F1'],\n",
    "    'Validation': [val_performance['accuracy'], val_performance['macro_f1']],\n",
    "    'Test': [test_metrics['accuracy'], test_metrics['macro_f1']],\n",
    "    'Difference': [test_metrics['accuracy'] - val_performance['accuracy'], \n",
    "                   test_metrics['macro_f1'] - val_performance['macro_f1']]\n",
    "})\n",
    "display(comparison_df)\n",
    "\n",
    "# Check for overfitting\n",
    "if abs(val_performance['macro_f1'] - test_metrics['macro_f1']) > 0.05:\n",
    "    print(\"⚠️ Warning: There may be some overfitting (>5% difference between validation and test performance)\")\n",
    "else:\n",
    "    print(\"✅ Model generalizes well (similar performance on validation and test sets)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514c1d6",
   "metadata": {},
   "source": [
    "## 7. Model Analysis and Insights\n",
    "\n",
    "Analyze the best model's behavior and extract insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Confusion Matrix Analysis\n",
    "cm = test_metrics['confusion_matrix']\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Normalized Confusion Matrix (Test Set)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify most confused classes\n",
    "confusion_pairs = []\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        if i != j:  # Skip diagonal elements\n",
    "            confusion_pairs.append((label_encoder.classes_[i], label_encoder.classes_[j], cm[i, j]))\n",
    "\n",
    "# Sort by confusion count (descending)\n",
    "confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"Most confused class pairs:\")\n",
    "for true_class, pred_class, count in confusion_pairs[:5]:\n",
    "    print(f\"  {true_class} predicted as {pred_class}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ad96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Feature Importance Analysis (for tree-based models)\n",
    "if best_model_name in [\"Random Forest\", \"Gradient Boosting\", \"XGBoost\"]:\n",
    "    # Get feature importances\n",
    "    if best_model_name == \"XGBoost\":\n",
    "        importances = best_model.feature_importances_\n",
    "    else:\n",
    "        importances = best_model.feature_importances_\n",
    "        \n",
    "    # Create DataFrame for visualization\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot top features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "    plt.title(f'Top 15 Feature Importances ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top 10 features and their importance\n",
    "    print(\"Top 10 most important features:\")\n",
    "    for i, (feature, importance) in enumerate(zip(feature_importance['Feature'].head(10), \n",
    "                                                 feature_importance['Importance'].head(10))):\n",
    "        print(f\"{i+1}. {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Per-class Performance Analysis\n",
    "class_report = test_metrics['class_report']\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "class_performance = pd.DataFrame({\n",
    "    'Class': list(class_report.keys())[:-3],  # Exclude 'accuracy', 'macro avg', 'weighted avg'\n",
    "    'Precision': [class_report[cls]['precision'] for cls in list(class_report.keys())[:-3]],\n",
    "    'Recall': [class_report[cls]['recall'] for cls in list(class_report.keys())[:-3]],\n",
    "    'F1-Score': [class_report[cls]['f1-score'] for cls in list(class_report.keys())[:-3]],\n",
    "    'Support': [class_report[cls]['support'] for cls in list(class_report.keys())[:-3]]\n",
    "})\n",
    "\n",
    "# Plot per-class metrics\n",
    "plt.figure(figsize=(14, 8))\n",
    "class_performance_melted = pd.melt(class_performance, id_vars=['Class', 'Support'], \n",
    "                                   value_vars=['Precision', 'Recall', 'F1-Score'],\n",
    "                                   var_name='Metric', value_name='Score')\n",
    "sns.barplot(x='Class', y='Score', hue='Metric', data=class_performance_melted)\n",
    "plt.title('Per-class Performance Metrics')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best and worst performing classes\n",
    "best_class = class_performance.sort_values('F1-Score', ascending=False).iloc[0]\n",
    "worst_class = class_performance.sort_values('F1-Score').iloc[0]\n",
    "\n",
    "print(f\"Best performing class: {best_class['Class']} (F1-Score: {best_class['F1-Score']:.4f})\")\n",
    "print(f\"Worst performing class: {worst_class['Class']} (F1-Score: {worst_class['F1-Score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbeb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 Error Analysis\n",
    "# Get misclassified samples\n",
    "y_pred = test_metrics['predictions']\n",
    "misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "\n",
    "if len(misclassified_indices) > 0:\n",
    "    print(f\"Number of misclassified samples: {len(misclassified_indices)} out of {len(y_test)} ({len(misclassified_indices)/len(y_test)*100:.2f}%)\")\n",
    "    \n",
    "    # Get original indices in the dataset\n",
    "    test_indices = np.arange(len(data))[len(X_train) + len(X_val):]\n",
    "    original_indices = test_indices[misclassified_indices]\n",
    "    \n",
    "    # Get misclassified samples with original values\n",
    "    misclassified_samples = data.iloc[original_indices]\n",
    "    \n",
    "    # Add true and predicted labels\n",
    "    misclassified_samples['true_label'] = [label_encoder.classes_[y_test[i]] for i in misclassified_indices]\n",
    "    misclassified_samples['predicted_label'] = [label_encoder.classes_[y_pred[i]] for i in misclassified_indices]\n",
    "    \n",
    "    # Display sample of misclassified instances\n",
    "    print(\"\\nSample of misclassified instances:\")\n",
    "    if 'commit_message' in misclassified_samples.columns:\n",
    "        display(misclassified_samples[['commit_message', 'true_label', 'predicted_label']].head(10))\n",
    "    else:\n",
    "        display(misclassified_samples[['true_label', 'predicted_label']].head(10))\n",
    "    \n",
    "    # Count misclassifications by class\n",
    "    misclass_counts = misclassified_samples.groupby(['true_label', 'predicted_label']).size().reset_index()\n",
    "    misclass_counts.columns = ['True Label', 'Predicted Label', 'Count']\n",
    "    misclass_counts = misclass_counts.sort_values('Count', ascending=False)\n",
    "    \n",
    "    print(\"\\nMost common misclassifications:\")\n",
    "    display(misclass_counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87970a2d",
   "metadata": {},
   "source": [
    "## 8. Model Calibration and Robustness\n",
    "\n",
    "Check how well-calibrated the model probabilities are and assess robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# 8.1 Probability Calibration Analysis\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    y_prob = best_model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    # For each class, calculate calibration curve\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        # Convert to binary problem (one-vs-rest)\n",
    "        y_true_binary = (y_test == i).astype(int)\n",
    "        y_prob_binary = y_prob[:, i]\n",
    "        \n",
    "        # Calculate calibration curve\n",
    "        prob_true, prob_pred = calibration_curve(y_true_binary, y_prob_binary, n_bins=10)\n",
    "        \n",
    "        # Plot calibration curve\n",
    "        plt.plot(prob_pred, prob_true, marker='o', linewidth=1, label=class_name)\n",
    "    \n",
    "    # Plot perfectly calibrated line\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    \n",
    "    plt.title('Calibration Curves (Reliability Diagram)')\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate Brier score (lower is better)\n",
    "    from sklearn.metrics import brier_score_loss\n",
    "    brier_scores = []\n",
    "    for i in range(len(label_encoder.classes_)):\n",
    "        y_true_binary = (y_test == i).astype(int)\n",
    "        y_prob_binary = y_prob[:, i]\n",
    "        brier = brier_score_loss(y_true_binary, y_prob_binary)\n",
    "        brier_scores.append(brier)\n",
    "    \n",
    "    print(\"Brier scores by class (lower is better):\")\n",
    "    for cls, score in zip(label_encoder.classes_, brier_scores):\n",
    "        print(f\"  {cls}: {score:.4f}\")\n",
    "    print(f\"Average Brier score: {np.mean(brier_scores):.4f}\")\n",
    "    \n",
    "    # Check if model is overconfident or underconfident\n",
    "    confidence = np.max(y_prob, axis=1)\n",
    "    mean_confidence = np.mean(confidence)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nMean prediction confidence: {mean_confidence:.4f}\")\n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if mean_confidence > accuracy + 0.1:\n",
    "        print(\"⚠️ Model may be overconfident (mean confidence > accuracy)\")\n",
    "    elif mean_confidence < accuracy - 0.1:\n",
    "        print(\"⚠️ Model may be underconfident (mean confidence < accuracy)\")\n",
    "    else:\n",
    "        print(\"✅ Model confidence is well-calibrated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6019e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Robustness Analysis\n",
    "# Simulate noise in the features to test robustness\n",
    "if len(X_test) > 20:\n",
    "    # Add Gaussian noise to features\n",
    "    noise_levels = [0.0, 0.05, 0.1, 0.2, 0.5]\n",
    "    noise_results = []\n",
    "    \n",
    "    for noise in noise_levels:\n",
    "        # Add noise to test set\n",
    "        X_test_noisy = X_test_scaled.copy()\n",
    "        if noise > 0:\n",
    "            X_test_noisy += np.random.normal(0, noise, X_test_noisy.shape)\n",
    "        \n",
    "        # Evaluate model on noisy data\n",
    "        noisy_metrics = evaluate_model(best_model, X_test_noisy, y_test, label_encoder)\n",
    "        noise_results.append({\n",
    "            'noise_level': noise,\n",
    "            'accuracy': noisy_metrics['accuracy'],\n",
    "            'macro_f1': noisy_metrics['macro_f1']\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    noise_df = pd.DataFrame(noise_results)\n",
    "    \n",
    "    # Plot impact of noise on performance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(noise_df['noise_level'], noise_df['accuracy'], marker='o', label='Accuracy')\n",
    "    plt.plot(noise_df['noise_level'], noise_df['macro_f1'], marker='s', label='Macro F1')\n",
    "    plt.title('Model Robustness to Feature Noise')\n",
    "    plt.xlabel('Noise Level (Standard Deviation)')\n",
    "    plt.ylabel('Performance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate degradation rate\n",
    "    initial_f1 = noise_df.iloc[0]['macro_f1']\n",
    "    max_noise_f1 = noise_df.iloc[-1]['macro_f1']\n",
    "    degradation = (initial_f1 - max_noise_f1) / initial_f1 * 100\n",
    "    \n",
    "    print(f\"Performance degradation at {noise_levels[-1]} noise level: {degradation:.2f}%\")\n",
    "    \n",
    "    if degradation > 50:\n",
    "        print(\"⚠️ Model is highly sensitive to noise\")\n",
    "    elif degradation > 25:\n",
    "        print(\"⚠️ Model shows moderate sensitivity to noise\")\n",
    "    else:\n",
    "        print(\"✅ Model is relatively robust to noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18c6a8",
   "metadata": {},
   "source": [
    "## 9. Model Deployment Preparation\n",
    "\n",
    "Save the best model and necessary preprocessing components for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa74207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Create a dictionary with all necessary components\n",
    "model_package = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'label_encoder': label_encoder,\n",
    "    'feature_columns': feature_cols,\n",
    "    'model_name': best_model_name,\n",
    "    'performance': {\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'macro_f1': test_metrics['macro_f1'],\n",
    "        'class_report': test_metrics['class_report']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the model package\n",
    "joblib.dump(model_package, 'best_model.bin')\n",
    "print(f\"Model saved to 'best_model.bin'\")\n",
    "\n",
    "# Create a simple prediction function for demonstration\n",
    "def predict_role(commit_message, files_changed=1, insertions=10, deletions=5):\n",
    "    \"\"\"\n",
    "    Simple function to demonstrate how to use the model for prediction.\n",
    "    \"\"\"\n",
    "    # This is a simplified version - in a real scenario, you would need to process\n",
    "    # the commit message using the same preprocessing steps as during training\n",
    "    # and ensure all feature values are available.\n",
    "    \n",
    "    # Load the model package\n",
    "    model_package = joblib.load('best_model.bin')\n",
    "    \n",
    "    # Create a feature vector (simplified - you'd need to extract all features)\n",
    "    features = np.zeros(len(model_package['feature_columns']))\n",
    "    \n",
    "    # Set known features (this is just a placeholder - real implementation would extract all features)\n",
    "    features[0] = len(commit_message)  # assuming first feature is message_length\n",
    "    features[6] = files_changed  # assuming files_changed is at index 6\n",
    "    features[7] = insertions  # assuming insertions is at index 7\n",
    "    features[8] = deletions  # assuming deletions is at index 8\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = model_package['scaler'].transform(features.reshape(1, -1))\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model_package['model'].predict(features_scaled)[0]\n",
    "    \n",
    "    # Get class label\n",
    "    predicted_role = model_package['label_encoder'].inverse_transform([prediction])[0]\n",
    "    \n",
    "    return predicted_role\n",
    "\n",
    "# Test the prediction function with a sample commit message\n",
    "sample_message = \"Fixed UI layout issues in the dashboard component\"\n",
    "print(f\"\\nSample prediction for commit message: \\\"{sample_message}\\\"\")\n",
    "print(f\"Predicted role: {predict_role(sample_message)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c13f96",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "Let's summarize our modeling process and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of the modeling process and results\n",
    "print(\"# Developer Role Classification Model Summary\\n\")\n",
    "\n",
    "print(\"## Dataset Overview\")\n",
    "print(f\"- Total samples: {len(data)}\")\n",
    "print(f\"- Number of features: {len(feature_cols)}\")\n",
    "print(f\"- Number of classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"- Class distribution: {dict(zip(label_encoder.classes_, np.bincount(data['role'].map(dict(zip(label_encoder.classes_, range(len(label_encoder.classes_)))))))}\")\n",
    "\n",
    "print(\"\\n## Model Comparison\")\n",
    "display(results_df)\n",
    "\n",
    "print(\"\\n## Best Model\")\n",
    "print(f\"- Selected model: {best_model_name}\")\n",
    "if best_model_name in [\"Random Forest\", \"Gradient Boosting\", \"XGBoost\"]:\n",
    "    print(\"- Top 5 features:\")\n",
    "    for i, (feature, importance) in enumerate(zip(feature_importance['Feature'].head(5), \n",
    "                                                 feature_importance['Importance'].head(5))):\n",
    "        print(f\"  {i+1}. {feature}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\n## Performance on Test Set\")\n",
    "print(f\"- Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"- Macro F1 Score: {test_metrics['macro_f1']:.4f}\")\n",
    "print(\"- Per-class F1 Scores:\")\n",
    "for i, cls in enumerate(label_encoder.classes_):\n",
    "    print(f\"  - {cls}: {test_metrics['f1'][i]:.4f}\")\n",
    "\n",
    "print(\"\\n## Conclusion\")\n",
    "if test_metrics['macro_f1'] > 0.9:\n",
    "    performance_assessment = \"excellent\"\n",
    "elif test_metrics['macro_f1'] > 0.8:\n",
    "    performance_assessment = \"good\"\n",
    "elif test_metrics['macro_f1'] > 0.7:\n",
    "    performance_assessment = \"moderate\"\n",
    "else:\n",
    "    performance_assessment = \"needs improvement\"\n",
    "\n",
    "print(f\"The {best_model_name} model achieved {performance_assessment} performance with a macro F1 score of {test_metrics['macro_f1']:.4f}.\")\n",
    "print(\"The model can effectively classify developer roles based on commit patterns and message content.\")\n",
    "\n",
    "if 'confusion_pairs' in locals() and len(confusion_pairs) > 0:\n",
    "    print(f\"\\nThe most commonly confused roles are {confusion_pairs[0][0]} and {confusion_pairs[0][1]}.\")\n",
    "\n",
    "print(\"\\nModel saved and ready for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
