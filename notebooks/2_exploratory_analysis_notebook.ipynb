{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d80eafe",
   "metadata": {},
   "source": [
    "# Developer Role Classification: Exploratory Data Analysis\n",
    "\n",
    "This notebook contains exploratory analysis revealing dataset properties, patterns, and potential risk factors for the Developer Role Classification project.\n",
    "\n",
    "We'll visually explore the data and document key observations to better understand:\n",
    "1. The distribution and balance of developer roles\n",
    "2. Key features and their relationships with roles\n",
    "3. Patterns in commit behavior across different roles\n",
    "4. Potential challenges and risk factors for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "# Print environment information for reproducibility\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"NLTK version: {nltk.__version__}\")\n",
    "\n",
    "# For reproducibility - Set fixed seeds everywhere\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Try to set seeds for other libraries if they're installed\n",
    "try:\n",
    "    from sklearn.utils import check_random_state\n",
    "    check_random_state(RANDOM_SEED)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc96e2",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Overview\n",
    "\n",
    "First, let's load our processed dataset and get an overview of its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed dataset\n",
    "try:\n",
    "    data = pd.read_csv('final_dataset.csv')\n",
    "    print(f\"Dataset loaded successfully with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Please run the preprocessing notebook first.\")\n",
    "    \n",
    "# Display the first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "display(data.head())\n",
    "\n",
    "# Get basic information about the dataset\n",
    "print(\"\\nDataset info:\")\n",
    "data.info()\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f441314",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis\n",
    "\n",
    "Let's examine the distribution of developer roles (our target variable) to assess class balance and representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54455b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of developer roles\n",
    "role_counts = data['role'].value_counts()\n",
    "print(\"Developer role distribution:\")\n",
    "display(role_counts)\n",
    "\n",
    "# Calculate percentage distribution\n",
    "role_percentages = data['role'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage distribution:\")\n",
    "display(role_percentages)\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Bar plot of role counts\n",
    "plt.subplot(1, 2, 1)\n",
    "ax = sns.countplot(y=data['role'], order=role_counts.index)\n",
    "plt.title('Distribution of Developer Roles')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Role')\n",
    "# Add count labels\n",
    "for i, count in enumerate(role_counts):\n",
    "    ax.text(count + 5, i, f\"{count} ({role_percentages[role_counts.index[i]]:.1f}%)\", va='center')\n",
    "\n",
    "# Pie chart of roles\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(role_counts, labels=role_counts.index, autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "plt.axis('equal')\n",
    "plt.title('Proportion of Developer Roles')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "min_class = role_counts.min()\n",
    "max_class = role_counts.max()\n",
    "imbalance_ratio = max_class / min_class\n",
    "print(f\"\\nClass imbalance ratio (largest/smallest): {imbalance_ratio:.2f}\")\n",
    "\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(\"⚠️ There is significant class imbalance. Consider using techniques like SMOTE for balancing.\")\n",
    "else:\n",
    "    print(\"✅ Class distribution is relatively balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51d501",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis\n",
    "\n",
    "### 3.1 Numerical Feature Distributions\n",
    "\n",
    "Let's examine the distribution of our numerical features to identify patterns and potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4eb5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features\n",
    "numerical_features = [col for col in data.columns if col not in ['role', 'commit_message', 'processed_message', 'clean_commit_message']]\n",
    "\n",
    "# Plot histograms for numerical features\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    sns.histplot(data[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plots for numerical features by developer role\n",
    "plt.figure(figsize=(15, 20))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(5, 3, i+1)\n",
    "    sns.boxplot(x='role', y=feature, data=data)\n",
    "    plt.title(f'{feature} by Role')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f0ff9",
   "metadata": {},
   "source": [
    "### 3.2 Feature Correlation Analysis\n",
    "\n",
    "Let's examine correlations between features to identify potential multicollinearity and the most important features for role prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d16536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = data[numerical_features].corr()\n",
    "\n",
    "# Plot heatmap of feature correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated feature pairs\n",
    "high_corr_threshold = 0.7\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) >= high_corr_threshold:\n",
    "            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"Highly correlated feature pairs (|r| >= 0.7):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"{feat1} and {feat2}: r = {corr:.2f}\")\n",
    "else:\n",
    "    print(\"No highly correlated feature pairs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276e341",
   "metadata": {},
   "source": [
    "### 3.3 Feature Importance for Role Prediction\n",
    "\n",
    "Let's analyze which features are most discriminative for different developer roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14277996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by role and calculate mean for each feature\n",
    "role_feature_means = data.groupby('role')[numerical_features].mean()\n",
    "display(role_feature_means)\n",
    "\n",
    "# Create heatmap of feature means by role\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(role_feature_means, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Average Feature Values by Developer Role')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate feature importance as the variance of feature means across roles\n",
    "feature_importance = role_feature_means.var(axis=0)\n",
    "feature_importance = feature_importance.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nFeature discriminative power (higher values indicate better role separation):\")\n",
    "display(feature_importance)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "feature_importance.plot(kind='bar')\n",
    "plt.title('Feature Discriminative Power for Role Classification')\n",
    "plt.ylabel('Variance of means across roles')\n",
    "plt.xlabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e75fa0",
   "metadata": {},
   "source": [
    "## 4. Text Content Analysis\n",
    "\n",
    "Let's analyze the commit messages to understand patterns and differences across roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865fade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have the commit messages\n",
    "if 'commit_message' not in data.columns:\n",
    "    print(\"Commit message column not found in the dataset.\")\n",
    "else:\n",
    "    # Download NLTK resources if needed\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "    \n",
    "    # Set up stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Function to clean and tokenize text\n",
    "    def process_text_for_wordcloud(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        # Convert to lowercase and remove special characters\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in text.split() if word not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    # Process commit messages for analysis\n",
    "    data['processed_for_cloud'] = data['commit_message'].apply(process_text_for_wordcloud)\n",
    "    \n",
    "    # Generate word clouds for each role\n",
    "    unique_roles = data['role'].unique()\n",
    "    \n",
    "    plt.figure(figsize=(18, 4 * len(unique_roles)))\n",
    "    for i, role in enumerate(unique_roles):\n",
    "        role_messages = ' '.join(data[data['role'] == role]['processed_for_cloud'])\n",
    "        \n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, height=400,\n",
    "            background_color='white',\n",
    "            colormap='viridis',\n",
    "            max_words=100,\n",
    "            contour_width=3,\n",
    "            contour_color='steelblue'\n",
    "        ).generate(role_messages)\n",
    "        \n",
    "        # Plot the word cloud\n",
    "        plt.subplot(len(unique_roles), 1, i+1)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Common Words in {role} Commit Messages')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract top keywords for each role\n",
    "    print(\"Top keywords by role:\")\n",
    "    \n",
    "    for role in unique_roles:\n",
    "        role_messages = ' '.join(data[data['role'] == role]['processed_for_cloud']).split()\n",
    "        role_word_counts = Counter(role_messages)\n",
    "        top_words = role_word_counts.most_common(10)\n",
    "        \n",
    "        print(f\"\\n{role}:\")\n",
    "        for word, count in top_words:\n",
    "            print(f\"  - {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8365153c",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction for Visualization\n",
    "\n",
    "Let's reduce the dimensionality of our feature space to visualize how well the roles separate in a lower-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f49db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select features for dimensionality reduction\n",
    "X_features = data[numerical_features].values\n",
    "y_roles = data['role'].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "pca_df = pd.DataFrame({\n",
    "    'PCA1': X_pca[:, 0],\n",
    "    'PCA2': X_pca[:, 1],\n",
    "    'Role': y_roles\n",
    "})\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='Role', data=pca_df, palette='viridis', s=100, alpha=0.7)\n",
    "plt.title('PCA Visualization of Developer Roles')\n",
    "plt.xlabel(f'Principal Component 1 (Explained Variance: {pca.explained_variance_ratio_[0]:.2%})')\n",
    "plt.ylabel(f'Principal Component 2 (Explained Variance: {pca.explained_variance_ratio_[1]:.2%})')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print explained variance\n",
    "print(f\"Total variance explained by first 2 PCA components: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Check feature contributions to principal components\n",
    "pca_components = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=numerical_features,\n",
    "    index=['PC1', 'PC2']\n",
    ")\n",
    "\n",
    "print(\"\\nPCA Components Feature Contributions:\")\n",
    "display(pca_components)\n",
    "\n",
    "# Visualize feature contributions to PCA components\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=pca_components.columns, y=pca_components.iloc[0])\n",
    "plt.title('Feature Contributions to PC1')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=pca_components.columns, y=pca_components.iloc[1])\n",
    "plt.title('Feature Contributions to PC2')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363628a",
   "metadata": {},
   "source": [
    "## 6. Text Feature Analysis with TF-IDF\n",
    "\n",
    "Let's explore how commit message content differs between roles using TF-IDF vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bf140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have processed commit messages\n",
    "if 'processed_message' not in data.columns and 'commit_message' in data.columns:\n",
    "    # Basic processing\n",
    "    data['processed_message'] = data['commit_message'].apply(process_text_for_wordcloud)\n",
    "\n",
    "if 'processed_message' in data.columns:\n",
    "    # Apply TF-IDF vectorization\n",
    "    tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(data['processed_message'])\n",
    "    \n",
    "    print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    \n",
    "    # Reduce dimensionality for visualization\n",
    "    svd = TruncatedSVD(n_components=2)\n",
    "    tfidf_2d = svd.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    tfidf_df = pd.DataFrame({\n",
    "        'SVD1': tfidf_2d[:, 0],\n",
    "        'SVD2': tfidf_2d[:, 1],\n",
    "        'Role': data['role'].values\n",
    "    })\n",
    "    \n",
    "    # Plot TF-IDF projection\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(x='SVD1', y='SVD2', hue='Role', data=tfidf_df, palette='viridis', s=100, alpha=0.7)\n",
    "    plt.title('TF-IDF Text Features Visualization (SVD)')\n",
    "    plt.xlabel(f'Component 1 (Explained Variance: {svd.explained_variance_ratio_[0]:.2%})')\n",
    "    plt.ylabel(f'Component 2 (Explained Variance: {svd.explained_variance_ratio_[1]:.2%})')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most important terms for each role\n",
    "    print(\"Most distinctive terms by role (TF-IDF):\")\n",
    "    \n",
    "    # Average TF-IDF values for each role\n",
    "    role_tfidf = {}\n",
    "    for role in data['role'].unique():\n",
    "        role_indices = data[data['role'] == role].index\n",
    "        role_tfidf[role] = tfidf_matrix[role_indices].mean(axis=0)\n",
    "    \n",
    "    # Get top terms for each role\n",
    "    for role, tfidf_avg in role_tfidf.items():\n",
    "        # Convert to array and get top indices\n",
    "        tfidf_array = np.asarray(tfidf_avg)[0]\n",
    "        top_indices = tfidf_array.argsort()[-10:][::-1]\n",
    "        top_terms = [(feature_names[idx], tfidf_array[idx]) for idx in top_indices]\n",
    "        \n",
    "        print(f\"\\n{role}:\")\n",
    "        for term, score in top_terms:\n",
    "            print(f\"  - {term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92bedc",
   "metadata": {},
   "source": [
    "## 7. Identifying Risk Factors\n",
    "\n",
    "Let's identify potential challenges and risk factors for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for class imbalance (again)\n",
    "print(\"Class Balance Assessment:\")\n",
    "role_percentages = data['role'].value_counts(normalize=True) * 100\n",
    "min_percentage = role_percentages.min()\n",
    "print(f\"Smallest class represents {min_percentage:.2f}% of the data\")\n",
    "\n",
    "if min_percentage < 15:\n",
    "    print(\"⚠️ HIGH RISK: Severe class imbalance detected\")\n",
    "elif min_percentage < 25:\n",
    "    print(\"⚠️ MEDIUM RISK: Moderate class imbalance detected\")\n",
    "else:\n",
    "    print(\"✅ LOW RISK: Classes are reasonably balanced\")\n",
    "\n",
    "# 2. Check for feature overlap between classes\n",
    "print(\"\\nFeature Separation Assessment:\")\n",
    "\n",
    "# Calculate feature overlap using coefficient of variation of means across roles\n",
    "feature_means = data.groupby('role')[numerical_features].mean()\n",
    "feature_std = data.groupby('role')[numerical_features].std()\n",
    "\n",
    "# Calculate coefficient of variation for each feature's means across roles\n",
    "cv_means = feature_means.std() / feature_means.mean()\n",
    "avg_cv = cv_means.mean()\n",
    "\n",
    "print(f\"Average coefficient of variation of feature means across roles: {avg_cv:.4f}\")\n",
    "if avg_cv < 0.2:\n",
    "    print(\"⚠️ HIGH RISK: Low feature separation between roles\")\n",
    "elif avg_cv < 0.5:\n",
    "    print(\"⚠️ MEDIUM RISK: Moderate feature separation between roles\")\n",
    "else:\n",
    "    print(\"✅ LOW RISK: Good feature separation between roles\")\n",
    "\n",
    "# 3. Check for potential overfitting risk (feature count vs. sample count)\n",
    "print(\"\\nOverfitting Risk Assessment:\")\n",
    "n_samples = len(data)\n",
    "n_features = len(numerical_features)\n",
    "samples_per_feature = n_samples / n_features\n",
    "\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Number of features: {n_features}\")\n",
    "print(f\"Samples per feature ratio: {samples_per_feature:.2f}\")\n",
    "\n",
    "if samples_per_feature < 10:\n",
    "    print(\"⚠️ HIGH RISK: Low sample-to-feature ratio may lead to overfitting\")\n",
    "elif samples_per_feature < 50:\n",
    "    print(\"⚠️ MEDIUM RISK: Moderate sample-to-feature ratio\")\n",
    "else:\n",
    "    print(\"✅ LOW RISK: Good sample-to-feature ratio\")\n",
    "\n",
    "# 4. Check for data quality issues\n",
    "print(\"\\nData Quality Assessment:\")\n",
    "\n",
    "# Missing values\n",
    "missing_values = data.isnull().sum().sum()\n",
    "print(f\"Total missing values: {missing_values}\")\n",
    "if missing_values > 0:\n",
    "    print(\"⚠️ RISK: Dataset contains missing values\")\n",
    "else:\n",
    "    print(\"✅ No missing values found\")\n",
    "\n",
    "# Check for role ambiguity using keyword overlaps\n",
    "if 'frontend_keywords' in data.columns and 'backend_keywords' in data.columns:\n",
    "    # Calculate average keyword counts by role\n",
    "    keyword_cols = [col for col in data.columns if col.endswith('_keywords')]\n",
    "    keyword_means = data.groupby('role')[keyword_cols].mean()\n",
    "    \n",
    "    print(\"\\nKeyword distribution by role:\")\n",
    "    display(keyword_means)\n",
    "    \n",
    "    # Check for ambiguity\n",
    "    ambiguity_scores = []\n",
    "    for role in keyword_means.index:\n",
    "        primary_keyword = role.lower() + \"_keywords\" if role.lower() + \"_keywords\" in keyword_cols else None\n",
    "        if primary_keyword:\n",
    "            # Get primary and secondary keyword values\n",
    "            primary_val = keyword_means.loc[role, primary_keyword]\n",
    "            secondary_vals = [keyword_means.loc[role, col] for col in keyword_cols if col != primary_keyword]\n",
    "            max_secondary = max(secondary_vals) if secondary_vals else 0\n",
    "            \n",
    "            # Calculate ambiguity score (higher means more ambiguous)\n",
    "            if primary_val > 0:\n",
    "                ambiguity = max_secondary / primary_val\n",
    "                ambiguity_scores.append((role, ambiguity))\n",
    "    \n",
    "    if ambiguity_scores:\n",
    "        print(\"\\nRole ambiguity scores (higher means more ambiguous):\")\n",
    "        for role, score in ambiguity_scores:\n",
    "            print(f\"{role}: {score:.2f}\")\n",
    "            if score > 0.8:\n",
    "                print(f\"⚠️ HIGH RISK: {role} role shows high keyword ambiguity\")\n",
    "            elif score > 0.5:\n",
    "                print(f\"⚠️ MEDIUM RISK: {role} role shows moderate keyword ambiguity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9a6e3f",
   "metadata": {},
   "source": [
    "## 8. Summary of Findings\n",
    "\n",
    "Let's summarize our key findings from the exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfc2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Summary of Exploratory Analysis\\n\")\n",
    "\n",
    "# 1. Dataset Overview\n",
    "print(\"## Dataset Overview\")\n",
    "print(f\"- Total number of commits: {len(data)}\")\n",
    "print(f\"- Number of unique developer roles: {data['role'].nunique()}\")\n",
    "role_counts = data['role'].value_counts()\n",
    "for role, count in role_counts.items():\n",
    "    print(f\"  - {role}: {count} commits ({count/len(data)*100:.1f}%)\")\n",
    "\n",
    "# 2. Key Features\n",
    "print(\"\\n## Key Discriminative Features\")\n",
    "# Get top 5 features by importance\n",
    "top_features = feature_importance.head(5)\n",
    "for feature, importance in top_features.items():\n",
    "    print(f\"- {feature}: {importance:.4f}\")\n",
    "\n",
    "# 3. Role Separation\n",
    "print(\"\\n## Role Separation\")\n",
    "pca_variance = sum(pca.explained_variance_ratio_)\n",
    "print(f\"- PCA explains {pca_variance:.1%} of variance with 2 components\")\n",
    "if pca_variance < 0.5:\n",
    "    print(\"- ⚠️ Roles are not well-separated in feature space\")\n",
    "else:\n",
    "    print(\"- ✅ Roles show reasonable separation in feature space\")\n",
    "\n",
    "# 4. Risk Factors\n",
    "print(\"\\n## Risk Factors\")\n",
    "# Class imbalance\n",
    "max_class = role_counts.max()\n",
    "min_class = role_counts.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(f\"- ⚠️ Class imbalance: largest/smallest class ratio = {imbalance_ratio:.2f}\")\n",
    "\n",
    "# Feature overlap\n",
    "if avg_cv < 0.3:\n",
    "    print(f\"- ⚠️ Low feature separation (CV = {avg_cv:.2f})\")\n",
    "\n",
    "# Overfitting risk\n",
    "if samples_per_feature < 20:\n",
    "    print(f\"- ⚠️ Risk of overfitting: only {samples_per_feature:.1f} samples per feature\")\n",
    "\n",
    "# 5. Distinctive Words by Role\n",
    "print(\"\\n## Distinctive Words by Role\")\n",
    "for role in data['role'].unique():\n",
    "    role_messages = ' '.join(data[data['role'] == role]['processed_for_cloud']).split()\n",
    "    role_word_counts = Counter(role_messages)\n",
    "    top_words = role_word_counts.most_common(5)\n",
    "    \n",
    "    print(f\"\\n### {role}:\")\n",
    "    for word, count in top_words:\n",
    "        print(f\"- {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b7312",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This exploratory analysis has provided valuable insights into our developer role classification dataset. We've identified key patterns, potential challenges, and risk factors that will inform our modeling approach.\n",
    "\n",
    "### Key Insights:\n",
    "1. **Feature Importance**: We've identified the most discriminative features for distinguishing between developer roles.\n",
    "2. **Text Patterns**: Different roles show distinct patterns in their commit messages, which can be leveraged for classification.\n",
    "3. **Role Separation**: PCA visualization shows how well (or poorly) the roles separate in the feature space.\n",
    "4. **Risk Factors**: We've identified potential challenges like class imbalance, feature overlap, and potential overfitting risks.\n",
    "\n",
    "### Next Steps:\n",
    "1. Address any class imbalance issues in the modeling phase\n",
    "2. Focus on the most discriminative features \n",
    "3. Consider feature engineering to improve role separation\n",
    "4. Implement techniques to mitigate identified risks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
